Policy Optimization for Decision Making
Supervised Deep Learning + Offline Reinforcement Learning (CQL)
ğŸ“Œ Project Overview

This repository implements an end-to-end decision optimization system combining:

1. A Supervised Deep Learning model (MLP)

Predicts loan repayment probability using borrower-level features.

2. An Offline Reinforcement Learning Policy (CQL / fallback)

Learns an approval strategy optimized for expected financial reward, trained only on historical logged data â€” no online interaction required.

Goal:

Evaluate which approachâ€”ML or RLâ€”leads to better real-world decision performance, not just predictive accuracy.

ğŸš€ Key Features

Clean preprocessing pipeline with reproducible feature engineering

Deep Learning classifier with threshold tuning

Offline RL dataset construction (MDP transitions)

Conservative Q-Learning (CQL) with robust fallbacks

Complete DL vs RL policy comparison

Automatic report generation (JSON, Markdown, LaTeX option)

Fully scripted, reproducible pipeline



ğŸ§  Data Pipeline Overview
1. Preprocessing

Cleans numeric/categorical fields

Removes post-outcome leakage features

Imputes missing values

Scales numeric columns

Saves a reusable preprocessing object (preprocessor_dl.joblib)

2. Supervised Model (MLP)

Two-layer deep neural network

Class-weighted BCE loss

Early stopping

Outputs probabilities â†’ threshold tuned on validation set

3. Offline RL (CQL)

RL dataset built from logged historical decisions

Transition tuples: (state, action, reward, next_state, terminal)

Reward function based on financial outcomes:

repayment â†’ + loan_amnt * int_rate  
default   â†’ â€“ loan_amnt  
reject    â†’ 0  


CQL attempted first; if unavailable, fallback logistic regression policy trained

4. Policy Comparison

Both DL and RL policies are evaluated using:

Metric	Description
AUC	Prediction discrimination (DL only)
F1 / Precision / Recall	Classification quality
Expected Reward	Business value of decisions
ğŸ“Š Results Summary
Model	AUC	F1	Avg Reward	Conclusion
Deep Learning (MLP)	~0.578	0.25	â€“0.04	Higher accuracy, poor reward
RL Policy (CQL/Fallback)	N/A	0.17	+0.036	Lower accuracy, best reward
ğŸ”¥ Insight:

The RL policy outperforms ML on business value, which is the real objective of decision-making systems.

âš™ï¸ How to Run the Entire Pipeline

Below are the exact commands used in workflow order.

1ï¸âƒ£ Preprocess Data
python scripts/data_prep.py --input data/raw/loan_data.csv --out data/processed

2ï¸âƒ£ Train Deep Learning Model
python scripts/train_supervised.py \
  --processed_dir data/processed \
  --artifacts_dir artifacts \
  --out_dir models/dl

3ï¸âƒ£ Build RL Dataset
python scripts/prepare_rl_dataset.py \
  --processed_dir data/processed \
  --out data/rl

4ï¸âƒ£ Train RL Agent (CQL or Fallback)
python scripts/train_rl_agent.py \
  --train data/rl/train_rl.npz \
  --val data/rl/val_rl.npz \
  --out models/rl \
  --n_epochs 50

5ï¸âƒ£ Compare DL vs RL Policies
python scripts/compare_policies.py \
  --test data/processed/test_processed.csv.gz \
  --val data/processed/val_processed.csv.gz \
  --dl_model models/dl/dl_model.pt \
  --dl_preproc models/preprocessor_dl.joblib \
  --rl_policy models/rl/rl_policy.pkl \
  --out reports/policy_comparison.json

6ï¸âƒ£ Generate Final Report
python scripts/final_report.py \
  --test data/processed/test_processed.csv.gz \
  --val data/processed/val_processed.csv.gz \
  --dl_model models/dl/dl_model.pt \
  --dl_preproc models/preprocessor_dl.joblib \
  --rl_policy models/rl/rl_policy.pkl \
  --comparison reports/policy_comparison.json \
  --out reports

ğŸ“¦ Requirements
python >= 3.9
numpy
pandas
torch
scikit-learn
joblib
matplotlib
d3rlpy (optional; fallback enabled)


If d3rlpy fails to install (common on Windows), the RL script automatically trains a logistic regression fallback.

ğŸ“ Key Takeaways

Classification accuracy â‰  optimal decisions

RL optimizes long-term reward, not prediction precision

Offline RL enables safe policy learning from historical logs

Threshold tuning significantly impacts DL policy reward

Combining DL probability models with RL decision logic is powerful

ğŸ“š Deliverables

This project includes:

Training scripts

Cleaned data artifacts

Trained supervised + RL models

Evaluation metrics

Final Markdown + LaTeX reports

Ready-to-run reproducible pipeline

ğŸ™‹ Author

Harsh Vardhan
Policy Optimization for Decision Making â€” 2025
